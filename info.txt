resnet<x>: 
	original resnet from https://arxiv.org/abs/1512.03385.

gaussian_resnet<x>_<kernel_size>_<std>:
	resnet with conv layers replaced by fixed gaussian kernels + 1x1 conv. 
	The very first conv is a normal convolution, because it is important to convolve RGB channels with a proper convolution, depthwise separable ones work worse.

fully_fixed_gaussian_resnet<x>_<kernel_size>_<std>:
	resnet with conv layers replaced by fixed gaussian kernels + 1x1 conv.
	All convolutions, including the very first one are replaced by gaussian + 1x1conv.

bilinear_resnet<x>:
	resnet with conv layers replaced by bilinear downsampling + 1x1 conv.


linear_encoder_5_512:
Linear encoder model implemented through FixedConvNet. 5 Blocks. Number of channels per block as follows:
    # block =             [ 0,  1,  2,   3,   4,   5]
    # input_channels =    [ 3, 16, 16,  32,  64, 128]
    # output_channels =   [16, 16, 32,  64, 128, 256]

gaussian_encoder_5_512_7_1.0:
Gaussian Blur model implemented through FixedConvNet. 5 blocks. Number of channels as in linear_encoder_5_512.
7_1.0 stand for kernel_size=7 for gaussian kernel and std=1.0


linear_encoder_old1:
contains the runs for the old version of linear encoder (implemented through DeepEncoder) with the number of channels per block as follows:

    # block =             [ 0,  1,  2,   3,   4,   5]
    # input_channels =    [ 3, 16, 16,  32,  64, 128]
    # output_channels =   [16, 16, 32,  64, 128, 256]

3,4,5 in 3_256, 4_256, 5_256 refers to the number of blocks used. 3_256 means that blocks 4 and 5 are not added.

linear_encoder_old1:
contains the runs for the old version of linear encoder (implemented through DeepEncoder) with the number of channels per block as follows:
    # block =             [ 0,  1,  2,   3,   4,   5]
    # input_channels =    [ 3, 16, 32,  64, 128, 256]
    # output_channels =   [16, 32, 64, 128, 256, 512]

All of the models are using 5 blocks. 
5_512_reduceon, 1cycle_0.01, 5_512_1cycle_0.1 refer to the LR schedule used.
1cycle has to be tested again.

